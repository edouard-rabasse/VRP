{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f713f5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# NotebookÂ : debug_deit_pipeline.ipynb\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Choisissez le chemin de votre fichier de config (.py)\n",
    "CFG_PATH = \"config/cfg_deit.py\"      # <- adaptez si besoin\n",
    "\n",
    "import sys, os, torch, torchvision, random, numpy as np\n",
    "sys.path.append(os.path.dirname(CFG_PATH))\n",
    "cfg = __import__(os.path.basename(CFG_PATH).replace(\".py\", \"\"))\n",
    "\n",
    "# ReproducibilitÃ© minimale\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED);  np.random.seed(SEED);  random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA:\", torch.cuda.is_available(), \"| device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc161283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform TRAIN  : Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "Transform TEST   : Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rabas\\anaconda3\\envs\\VRP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from src.load_model import load_model\n",
    "from src.data_loader_mask import load_data_train_test\n",
    "from src.transform import image_transform_train, image_transform_test, mask_transform\n",
    "\n",
    "print(\"Transform TRAIN  :\", image_transform_train(size=cfg.image_size))\n",
    "print(\"Transform TEST   :\", image_transform_test(size=cfg.image_size))\n",
    "\n",
    "\n",
    "# â¬‡ï¸Ž same params que votre script principal\n",
    "# train_loader, test_loader = load_data_train_test(\n",
    "#     train_original_path = cfg.train_original_path,\n",
    "#     test_original_path  = cfg.test_original_path,\n",
    "#     train_modified_path = cfg.train_modified_path,\n",
    "#     test_modified_path  = cfg.test_modified_path,\n",
    "#     mask_path_train     = cfg.train_mask_path,\n",
    "#     mask_path_test      = cfg.test_mask_path,\n",
    "#     batch_size          = cfg.batch_size,\n",
    "#     image_transform_train=image_transform_train(size=cfg.image_size),\n",
    "#     image_transform_test =image_transform_test(size=cfg.image_size),\n",
    "#     mask_transform_train =mask_transform(size=cfg.image_size),\n",
    "#     mask_transform_test  =mask_transform(size=cfg.image_size),\n",
    "#     image_size          = cfg.image_size,\n",
    "#     num_workers         = 0,\n",
    "# )\n",
    "\n",
    "# # IMPORTANT : loader sans mÃ©lange pour comparer indice par indice\n",
    "# train_loader.shuffle = False\n",
    "# test_loader.shuffle  = False\n",
    "\n",
    "model = load_model(cfg.model_name, device=device, cfg=cfg).to(device)\n",
    "model.eval();  # backbone + head figÃ©s\n",
    "print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871b27a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dÃ©terministeÂ : 118 images\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 3bis. Dataset & DataLoader dÃ©terministes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) Transformations \"neutres\" : aucun Random*, mÃªme normalisation que DeiT\n",
    "from src.transform import image_transform_test, mask_transform  # test = resize + ToTensor + Normalize\n",
    "\n",
    "deterministic_img_tf = image_transform_test(size=cfg.image_size)  # â‰ƒ resize + ToTensor + Normalize\n",
    "deterministic_msk_tf = mask_transform(size=cfg.image_size)\n",
    "\n",
    "# 2) On reconstruit explicitement le Dataset d'entraÃ®nement\n",
    "from src.data_loader_mask import CustomDataset  # â† adapte si ton dataset a un autre nom\n",
    "train_dataset_det = CustomDataset(\n",
    "    original_dir   = cfg.train_original_path,\n",
    "    modified_dir  = cfg.train_modified_path,\n",
    "    mask_dir   = cfg.train_mask_path,\n",
    "    image_transform    = deterministic_img_tf,\n",
    "    mask_transform     = deterministic_msk_tf,\n",
    "    # image_size  = cfg.image_size\n",
    ")\n",
    "\n",
    "# 3) DataLoader sans mÃ©lange\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "train_loader_det = DataLoader(\n",
    "    train_dataset_det,\n",
    "    batch_size     = cfg.batch_size,\n",
    "    sampler        = SequentialSampler(train_dataset_det),  # ðŸ”‘ ordre fixe\n",
    "    num_workers    = 0,\n",
    "    pin_memory     = False\n",
    ")\n",
    "\n",
    "print(f\"Dataset dÃ©terministeÂ : {len(train_dataset_det)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261bbf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] deit_tiny.py loaded\n",
      "Î”max entre embeddings live & prÃ©â€‘calc (dÃ©terministe) : 2.279e+00\n",
      "âŒ  Toujours un Ã©cart : vÃ©rifie encore model.eval() ou la Normalization.\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 4bis. PrÃ©â€‘compute embeddings sans shuffle, puis comparaison â”€â”€â”€â”€â”€â”€\n",
    "from models.deit_tiny import precompute_deit_tiny_features\n",
    "\n",
    "# 1) (rÃ©)gÃ©nÃ¨re les embeddings dÃ©terministes\n",
    "train_feats_det = precompute_deit_tiny_features(model, train_loader_det, device=device)\n",
    "\n",
    "# 2) On prÃ©lÃ¨ve un batch du loader dÃ©terministe\n",
    "imgs, labels, _ = next(iter(train_loader_det))\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats_live = model.forward_features(imgs)[:, 0]       # (B, 192)\n",
    "\n",
    "# 3) Embeddings prÃ©â€‘calculÃ©s, mÃªmes indices 0â€¦Bâ€‘1\n",
    "feats_pre = torch.stack([train_feats_det[i][0] for i in range(imgs.size(0))])[:, 0, :]  # (B,192)\n",
    "\n",
    "# 4) DiffÃ©rence\n",
    "delta = (feats_live - feats_pre).abs().max()\n",
    "print(f\"Î”max entre embeddings live & prÃ©â€‘calc (dÃ©terministe) : {delta:.3e}\")\n",
    "\n",
    "if delta < 1e-3:\n",
    "    print(\"âœ…  Pipelines maintenant identiques.\")\n",
    "else:\n",
    "    print(\"âŒ  Toujours un Ã©cart : vÃ©rifie encore model.eval() ou la Normalization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1331f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy tÃªte (batch) : 87.50%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model.head(feats_live.to(device))\n",
    "preds = logits.argmax(1)\n",
    "acc_batch = (preds.cpu() == labels.cpu()).float().mean()\n",
    "print(f\"Accuracy tÃªte (batch) : {acc_batch:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69f16ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. images brutes  : 51.69491525423729 %\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ok \u001b[38;5;241m/\u001b[39m tot\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc. images brutes  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_from_images(model, train_loader)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc. embeddings pre :\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_from_feats(model, train_feats_ds)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36maccuracy_from_feats\u001b[1;34m(model, feats_ds)\u001b[0m\n\u001b[0;32m     13\u001b[0m ok \u001b[38;5;241m=\u001b[39m tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feats, labels \u001b[38;5;129;01min\u001b[39;00m feats_ds:\n\u001b[0;32m     16\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhead(feats\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     17\u001b[0m         ok  \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logits\u001b[38;5;241m.\u001b[39margmax() \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def accuracy_from_images(model, loader):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _ in loader:\n",
    "            feats = model.forward_features(imgs.to(device))[:, 0]\n",
    "            logits = model.head(feats)\n",
    "            ok  += (logits.argmax(1).cpu() == labels).sum().item()\n",
    "            tot += labels.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "def accuracy_from_feats(model, feats_ds):\n",
    "    ok = tot = 0\n",
    "    with torch.no_grad():\n",
    "        for feats, labels in feats_ds:\n",
    "            logits = model.head(feats.to(device))\n",
    "            ok  += (logits.argmax() == labels.to(device)).item()\n",
    "            tot += 1\n",
    "    return ok / tot\n",
    "\n",
    "print(\"Acc. images brutes  :\", accuracy_from_images(model, train_loader)*100, \"%\")\n",
    "print(\"Acc. embeddings pre :\", accuracy_from_feats(model, train_feats_ds)*100, \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
